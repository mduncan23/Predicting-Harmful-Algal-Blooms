{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Importing-the-Data\" data-toc-modified-id=\"Importing-the-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Importing the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metadata-File\" data-toc-modified-id=\"Metadata-File-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><code>Metadata</code> File</a></span></li><li><span><a href=\"#train_labels-File\" data-toc-modified-id=\"train_labels-File-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><code>train_labels</code> File</a></span></li><li><span><a href=\"#Prepping-the-data-for-the-Satellite-imagery-analysis.\" data-toc-modified-id=\"Prepping-the-data-for-the-Satellite-imagery-analysis.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Prepping the data for the Satellite imagery analysis.</a></span></li><li><span><a href=\"#Setting-up-the-DataFrame\" data-toc-modified-id=\"Setting-up-the-DataFrame-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Setting up the DataFrame</a></span></li></ul></li><li><span><a href=\"#Pulling-in-All-of-the-Data\" data-toc-modified-id=\"Pulling-in-All-of-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pulling in All of the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pulling-in-the-first-half-of-the-data.\" data-toc-modified-id=\"Pulling-in-the-first-half-of-the-data.-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Pulling in the first half of the data.</a></span></li></ul></li><li><span><a href=\"#Checking-results-of-first-pull-(a-lot-more-missing-than-expected)\" data-toc-modified-id=\"Checking-results-of-first-pull-(a-lot-more-missing-than-expected)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Checking results of first pull (a lot more missing than expected)</a></span></li><li><span><a href=\"#first-attempt-at-pulling-in-data-(could-also-include-landsat-7)\" data-toc-modified-id=\"first-attempt-at-pulling-in-data-(could-also-include-landsat-7)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>first attempt at pulling in data (could also include landsat 7)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pulling-in-the-second-half-of-the-data\" data-toc-modified-id=\"Pulling-in-the-second-half-of-the-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Pulling in the second half of the data</a></span></li><li><span><a href=\"#Pulling-in-the-third-set-of-data\" data-toc-modified-id=\"Pulling-in-the-third-set-of-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Pulling in the third set of data</a></span></li><li><span><a href=\"#Creating-a-Full-DataFrame\" data-toc-modified-id=\"Creating-a-Full-DataFrame-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Creating a Full DataFrame</a></span></li></ul></li><li><span><a href=\"#Test-Data\" data-toc-modified-id=\"Test-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Test Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pulling-the-the-Data-in-Batches-and-Saving-to-.pkl\" data-toc-modified-id=\"Pulling-the-the-Data-in-Batches-and-Saving-to-.pkl-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Pulling the the Data in Batches and Saving to .pkl</a></span></li><li><span><a href=\"#Reading-in-the-Saved-Test-Data\" data-toc-modified-id=\"Reading-in-the-Saved-Test-Data-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Reading in the Saved Test Data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running main notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:02.231334Z",
     "start_time": "2023-02-06T00:30:59.975603Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import geopy.distance as distance\n",
    "\n",
    "import planetary_computer as pc\n",
    "from pystac_client import Client\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# from keras.utils import load_img, img_to_array\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import rioxarray\n",
    "import cv2\n",
    "import odc.stac\n",
    "import tempfile\n",
    "import rasterio\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import functions\n",
    "import functions2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Metadata` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:03.845330Z",
     "start_time": "2023-02-06T00:31:03.820333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading in the data and bringing in date as datetime dtype\n",
    "metadata = pd.read_csv('Data/metadata.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_labels` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:05.249834Z",
     "start_time": "2023-02-06T00:31:05.236328Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('Data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the data for the Satellite imagery analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:09.533537Z",
     "start_time": "2023-02-06T00:31:09.524537Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_df = metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:12.106551Z",
     "start_time": "2023-02-06T00:31:12.092671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabm</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aacd</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaee</td>\n",
       "      <td>35.487000</td>\n",
       "      <td>-79.062133</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaff</td>\n",
       "      <td>38.049471</td>\n",
       "      <td>-99.827001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23566</th>\n",
       "      <td>zzwo</td>\n",
       "      <td>39.792190</td>\n",
       "      <td>-99.971050</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23567</th>\n",
       "      <td>zzwq</td>\n",
       "      <td>35.794000</td>\n",
       "      <td>-79.012551</td>\n",
       "      <td>2015-03-24</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23568</th>\n",
       "      <td>zzyb</td>\n",
       "      <td>35.742000</td>\n",
       "      <td>-79.238600</td>\n",
       "      <td>2016-11-21</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23570 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date  split\n",
       "0      aabm  39.080319  -86.430867 2018-05-14  train\n",
       "1      aabn  36.559700 -121.510000 2016-08-31   test\n",
       "2      aacd  35.875083  -78.878434 2020-11-19  train\n",
       "3      aaee  35.487000  -79.062133 2016-08-24  train\n",
       "4      aaff  38.049471  -99.827001 2019-07-23  train\n",
       "...     ...        ...         ...        ...    ...\n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02   test\n",
       "23566  zzwo  39.792190  -99.971050 2017-06-19  train\n",
       "23567  zzwq  35.794000  -79.012551 2015-03-24  train\n",
       "23568  zzyb  35.742000  -79.238600 2016-11-21  train\n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31   test\n",
       "\n",
       "[23570 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:14.435316Z",
     "start_time": "2023-02-06T00:31:14.420302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    17060\n",
       "test      6510\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:15.060789Z",
     "start_time": "2023-02-06T00:31:15.044277Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_train = sat_df[sat_df['split'] == 'train'].copy()\n",
    "sat_test = sat_df[sat_df['split'] == 'test'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing back in the labels for sat_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:17.797622Z",
     "start_time": "2023-02-06T00:31:17.780607Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_train = sat_train.merge(train_labels, on='uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I use a custom function to add a date range that the satellites can interpret and also include bounding boxes to later manipulate the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:31:44.036303Z",
     "start_time": "2023-02-06T00:31:22.260645Z"
    }
   },
   "outputs": [],
   "source": [
    "functions.get_important_info(sat_train, dist=31, big_crop_dist=3000, small_crop_dist=500, tiny_crop_dist=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:32:19.782188Z",
     "start_time": "2023-02-06T00:32:19.764682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>date_range</th>\n",
       "      <th>bbox</th>\n",
       "      <th>big_crop_bbox</th>\n",
       "      <th>small_crop_bbox</th>\n",
       "      <th>tiny_crop_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabm</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1</td>\n",
       "      <td>585.0</td>\n",
       "      <td>2018-04-29/2018-05-14</td>\n",
       "      <td>[-87.00742888244132, 38.63091417147125, -85.85...</td>\n",
       "      <td>[-86.46553737052635, 39.05329612116674, -86.39...</td>\n",
       "      <td>[-86.43664511758249, 39.07581525298953, -86.42...</td>\n",
       "      <td>[-86.43202235685135, 39.079418305988106, -86.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aacd</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>290.0</td>\n",
       "      <td>2020-11-04/2020-11-19</td>\n",
       "      <td>[-79.43088170919651, 35.425434522510464, -78.3...</td>\n",
       "      <td>[-78.91165478658658, 35.84804560208817, -78.84...</td>\n",
       "      <td>[-78.88397103218583, 35.8705769758293, -78.872...</td>\n",
       "      <td>[-78.87954163128398, 35.87418198776951, -78.87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaee</td>\n",
       "      <td>35.487000</td>\n",
       "      <td>-79.062133</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>2016-08-09/2016-08-24</td>\n",
       "      <td>[-79.61191193921022, 35.03732231399556, -78.51...</td>\n",
       "      <td>[-79.09519299105324, 35.459960615407574, -79.0...</td>\n",
       "      <td>[-79.06764296370947, 35.48249344433146, -79.05...</td>\n",
       "      <td>[-79.06323495914324, 35.48609868913609, -79.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaff</td>\n",
       "      <td>38.049471</td>\n",
       "      <td>-99.827001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>3</td>\n",
       "      <td>111825.0</td>\n",
       "      <td>2019-07-08/2019-07-23</td>\n",
       "      <td>[-100.3953854756163, 37.59998670596361, -99.25...</td>\n",
       "      <td>[-99.86118001864864, 38.02244310076497, -99.79...</td>\n",
       "      <td>[-99.83269759502433, 38.044966208779, -99.8213...</td>\n",
       "      <td>[-99.82814040700623, 38.048569898032675, -99.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aafl</td>\n",
       "      <td>39.474744</td>\n",
       "      <td>-86.898353</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>4</td>\n",
       "      <td>2017313.0</td>\n",
       "      <td>2021-08-08/2021-08-23</td>\n",
       "      <td>[-87.47815708269697, 39.02536958186914, -86.31...</td>\n",
       "      <td>[-86.93321866191961, 39.44772288780534, -86.86...</td>\n",
       "      <td>[-86.9041639439351, 39.47024049004548, -86.892...</td>\n",
       "      <td>[-86.89951518878857, 39.473843298288934, -86.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid   latitude  longitude       date  split   region  severity    density  \\\n",
       "0  aabm  39.080319 -86.430867 2018-05-14  train  midwest         1      585.0   \n",
       "1  aacd  35.875083 -78.878434 2020-11-19  train    south         1      290.0   \n",
       "2  aaee  35.487000 -79.062133 2016-08-24  train    south         1     1614.0   \n",
       "3  aaff  38.049471 -99.827001 2019-07-23  train  midwest         3   111825.0   \n",
       "4  aafl  39.474744 -86.898353 2021-08-23  train  midwest         4  2017313.0   \n",
       "\n",
       "              date_range                                               bbox  \\\n",
       "0  2018-04-29/2018-05-14  [-87.00742888244132, 38.63091417147125, -85.85...   \n",
       "1  2020-11-04/2020-11-19  [-79.43088170919651, 35.425434522510464, -78.3...   \n",
       "2  2016-08-09/2016-08-24  [-79.61191193921022, 35.03732231399556, -78.51...   \n",
       "3  2019-07-08/2019-07-23  [-100.3953854756163, 37.59998670596361, -99.25...   \n",
       "4  2021-08-08/2021-08-23  [-87.47815708269697, 39.02536958186914, -86.31...   \n",
       "\n",
       "                                       big_crop_bbox  \\\n",
       "0  [-86.46553737052635, 39.05329612116674, -86.39...   \n",
       "1  [-78.91165478658658, 35.84804560208817, -78.84...   \n",
       "2  [-79.09519299105324, 35.459960615407574, -79.0...   \n",
       "3  [-99.86118001864864, 38.02244310076497, -99.79...   \n",
       "4  [-86.93321866191961, 39.44772288780534, -86.86...   \n",
       "\n",
       "                                     small_crop_bbox  \\\n",
       "0  [-86.43664511758249, 39.07581525298953, -86.42...   \n",
       "1  [-78.88397103218583, 35.8705769758293, -78.872...   \n",
       "2  [-79.06764296370947, 35.48249344433146, -79.05...   \n",
       "3  [-99.83269759502433, 38.044966208779, -99.8213...   \n",
       "4  [-86.9041639439351, 39.47024049004548, -86.892...   \n",
       "\n",
       "                                      tiny_crop_bbox  \n",
       "0  [-86.43202235685135, 39.079418305988106, -86.4...  \n",
       "1  [-78.87954163128398, 35.87418198776951, -78.87...  \n",
       "2  [-79.06323495914324, 35.48609868913609, -79.06...  \n",
       "3  [-99.82814040700623, 38.048569898032675, -99.8...  \n",
       "4  [-86.89951518878857, 39.473843298288934, -86.8...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in All of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the API is sometimes unstable. I will be pulling over the data in two large batches with several smaller batches making up the larger batches. I am splitting the data into batches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:32:23.107870Z",
     "start_time": "2023-02-06T00:32:23.092294Z"
    }
   },
   "outputs": [],
   "source": [
    "all_train = list(np.arange(0, len(sat_train), 853))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:32:23.876024Z",
     "start_time": "2023-02-06T00:32:23.860011Z"
    }
   },
   "outputs": [],
   "source": [
    "all_train.append(17060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:32:25.958891Z",
     "start_time": "2023-02-06T00:32:25.956333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 853,\n",
       " 1706,\n",
       " 2559,\n",
       " 3412,\n",
       " 4265,\n",
       " 5118,\n",
       " 5971,\n",
       " 6824,\n",
       " 7677,\n",
       " 8530,\n",
       " 9383,\n",
       " 10236,\n",
       " 11089,\n",
       " 11942,\n",
       " 12795,\n",
       " 13648,\n",
       " 14501,\n",
       " 15354,\n",
       " 16207,\n",
       " 17060]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:32:28.366450Z",
     "start_time": "2023-02-06T00:32:28.348335Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "for slice in range(1, len(all_train)):\n",
    "    train_dict[f\"sat_train_{slice}\"] = sat_train[all_train[slice-1]:all_train[slice]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:32:29.774489Z",
     "start_time": "2023-02-06T00:32:29.772397Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dict_key_list = list(train_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:32:30.429959Z",
     "start_time": "2023-02-06T00:32:30.420452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>date_range</th>\n",
       "      <th>bbox</th>\n",
       "      <th>big_crop_bbox</th>\n",
       "      <th>small_crop_bbox</th>\n",
       "      <th>tiny_crop_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabm</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1</td>\n",
       "      <td>585.0</td>\n",
       "      <td>2018-04-29/2018-05-14</td>\n",
       "      <td>[-87.00742888244132, 38.63091417147125, -85.85...</td>\n",
       "      <td>[-86.46553737052635, 39.05329612116674, -86.39...</td>\n",
       "      <td>[-86.43664511758249, 39.07581525298953, -86.42...</td>\n",
       "      <td>[-86.43202235685135, 39.079418305988106, -86.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aacd</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>290.0</td>\n",
       "      <td>2020-11-04/2020-11-19</td>\n",
       "      <td>[-79.43088170919651, 35.425434522510464, -78.3...</td>\n",
       "      <td>[-78.91165478658658, 35.84804560208817, -78.84...</td>\n",
       "      <td>[-78.88397103218583, 35.8705769758293, -78.872...</td>\n",
       "      <td>[-78.87954163128398, 35.87418198776951, -78.87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaee</td>\n",
       "      <td>35.487000</td>\n",
       "      <td>-79.062133</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>2016-08-09/2016-08-24</td>\n",
       "      <td>[-79.61191193921022, 35.03732231399556, -78.51...</td>\n",
       "      <td>[-79.09519299105324, 35.459960615407574, -79.0...</td>\n",
       "      <td>[-79.06764296370947, 35.48249344433146, -79.05...</td>\n",
       "      <td>[-79.06323495914324, 35.48609868913609, -79.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaff</td>\n",
       "      <td>38.049471</td>\n",
       "      <td>-99.827001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>3</td>\n",
       "      <td>111825.0</td>\n",
       "      <td>2019-07-08/2019-07-23</td>\n",
       "      <td>[-100.3953854756163, 37.59998670596361, -99.25...</td>\n",
       "      <td>[-99.86118001864864, 38.02244310076497, -99.79...</td>\n",
       "      <td>[-99.83269759502433, 38.044966208779, -99.8213...</td>\n",
       "      <td>[-99.82814040700623, 38.048569898032675, -99.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aafl</td>\n",
       "      <td>39.474744</td>\n",
       "      <td>-86.898353</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>4</td>\n",
       "      <td>2017313.0</td>\n",
       "      <td>2021-08-08/2021-08-23</td>\n",
       "      <td>[-87.47815708269697, 39.02536958186914, -86.31...</td>\n",
       "      <td>[-86.93321866191961, 39.44772288780534, -86.86...</td>\n",
       "      <td>[-86.9041639439351, 39.47024049004548, -86.892...</td>\n",
       "      <td>[-86.89951518878857, 39.473843298288934, -86.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid   latitude  longitude       date  split   region  severity    density  \\\n",
       "0  aabm  39.080319 -86.430867 2018-05-14  train  midwest         1      585.0   \n",
       "1  aacd  35.875083 -78.878434 2020-11-19  train    south         1      290.0   \n",
       "2  aaee  35.487000 -79.062133 2016-08-24  train    south         1     1614.0   \n",
       "3  aaff  38.049471 -99.827001 2019-07-23  train  midwest         3   111825.0   \n",
       "4  aafl  39.474744 -86.898353 2021-08-23  train  midwest         4  2017313.0   \n",
       "\n",
       "              date_range                                               bbox  \\\n",
       "0  2018-04-29/2018-05-14  [-87.00742888244132, 38.63091417147125, -85.85...   \n",
       "1  2020-11-04/2020-11-19  [-79.43088170919651, 35.425434522510464, -78.3...   \n",
       "2  2016-08-09/2016-08-24  [-79.61191193921022, 35.03732231399556, -78.51...   \n",
       "3  2019-07-08/2019-07-23  [-100.3953854756163, 37.59998670596361, -99.25...   \n",
       "4  2021-08-08/2021-08-23  [-87.47815708269697, 39.02536958186914, -86.31...   \n",
       "\n",
       "                                       big_crop_bbox  \\\n",
       "0  [-86.46553737052635, 39.05329612116674, -86.39...   \n",
       "1  [-78.91165478658658, 35.84804560208817, -78.84...   \n",
       "2  [-79.09519299105324, 35.459960615407574, -79.0...   \n",
       "3  [-99.86118001864864, 38.02244310076497, -99.79...   \n",
       "4  [-86.93321866191961, 39.44772288780534, -86.86...   \n",
       "\n",
       "                                     small_crop_bbox  \\\n",
       "0  [-86.43664511758249, 39.07581525298953, -86.42...   \n",
       "1  [-78.88397103218583, 35.8705769758293, -78.872...   \n",
       "2  [-79.06764296370947, 35.48249344433146, -79.05...   \n",
       "3  [-99.83269759502433, 38.044966208779, -99.8213...   \n",
       "4  [-86.9041639439351, 39.47024049004548, -86.892...   \n",
       "\n",
       "                                      tiny_crop_bbox  \n",
       "0  [-86.43202235685135, 39.079418305988106, -86.4...  \n",
       "1  [-78.87954163128398, 35.87418198776951, -78.87...  \n",
       "2  [-79.06323495914324, 35.48609868913609, -79.06...  \n",
       "3  [-99.82814040700623, 38.048569898032675, -99.8...  \n",
       "4  [-86.89951518878857, 39.473843298288934, -86.8...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['sat_train_1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the first half of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is my second attempt at pulling in all data. I am doing it a second time to ensure that I don't accidentally pull in any old Landsat data. For the competition, I can only use LandSat 8 and 9. I have rewritten the code to be cleaner and exclude any old landsat data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 1 & 2 (double check this one, about 211 datapoints missing from viz data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:19:44.055414Z",
     "start_time": "2023-02-05T23:52:09.111381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853 attempts. 18 failures.\n",
      "853 attempted. 27 failures.\n",
      "853 attempts. 17 failures.\n",
      "853 attempted. 24 failures.\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN' 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'><html xmlns='http://www.w3.org/1999/xhtml'><head><meta content='text/html; charset=utf-8' http-equiv='content-type'/><style type='text/css'>body {font-family:Arial; margin-left:40px; }img  { border:0 none; }#content { margin-left: auto; margin-right: auto }#message h2 { font-size: 20px; font-weight: normal; color: #000000; margin: 34px 0px 0px 0px }#message p  { font-size: 13px; color: #000000; margin: 7px 0px 0px0px}#errorref { font-size: 11px; color: #737373; margin-top: 41px }</style><title>Service unavailable</title></head><body><div id='content'><div id='message'><h2>Our services aren't available right now</h2><p>We're working to restore all services as soon as possible. Please check back soon.</p></div><div id='errorref'><span>0fUfgYwAAAADt2iLbfsajSq4Ibkcn51yaTU5aMjIxMDYwNjE0MDI5ADkyN2FiZmE2LTE5ZjYtNGFmMS1hMDlkLWM5NTlkOWExZTY0NA==</span></div></div></body></html>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m first_three_results_dict\u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m train_dict_key_list[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     first_three_results_dict[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfunctions2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_get_sat_to_features2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\flatiron\\important_repo\\final_project\\Detecting-Harmful-Algal-Blooms\\functions2.py:585\u001b[0m, in \u001b[0;36mtry_get_sat_to_features2\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    580\u001b[0m catalog \u001b[38;5;241m=\u001b[39m Client\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://planetarycomputer.microsoft.com/api/stac/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, modifier\u001b[38;5;241m=\u001b[39mpc\u001b[38;5;241m.\u001b[39msign_inplace\n\u001b[0;32m    582\u001b[0m )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# get sat info\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m satelite_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtry_get_sat_info2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# pick best sat\u001b[39;00m\n\u001b[0;32m    588\u001b[0m single_df \u001b[38;5;241m=\u001b[39m pick_best_sat(df, satelite_dict)\n",
      "File \u001b[1;32m~\\Desktop\\flatiron\\important_repo\\final_project\\Detecting-Harmful-Algal-Blooms\\functions2.py:300\u001b[0m, in \u001b[0;36mtry_get_sat_info2\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    290\u001b[0m     search \u001b[38;5;241m=\u001b[39m catalog\u001b[38;5;241m.\u001b[39msearch(collections\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlandsat-8-c2-l2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlandsat-9-c2-l2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentinel-2-l2a\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    291\u001b[0m                             bbox\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    292\u001b[0m                             datetime\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_range\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    293\u001b[0m                             query\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meo:cloud_cover\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlt\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m100\u001b[39m}}\n\u001b[0;32m    294\u001b[0m )\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;66;03m# Going through Satellite info\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# search for sat images and create a dataframe with results for one sample\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     search_items \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    303\u001b[0m     pic_details \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pic \u001b[38;5;129;01min\u001b[39;00m search_items:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pystac_client\\item_search.py:745\u001b[0m, in \u001b[0;36mItemSearch.item_collection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;124;03mGet the matching items as a :py:class:`pystac.ItemCollection`.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03mReturn:\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;124;03m    ItemCollection: The item collection\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;66;03m# Bypass the cache here, so that we can pass __preserve_dict__\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# without mutating what's in the cache.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m feature_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_collection_as_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# already signed in item_collection_as_dict\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ItemCollection\u001b[38;5;241m.\u001b[39mfrom_dict(\n\u001b[0;32m    748\u001b[0m     feature_collection, preserve_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\n\u001b[0;32m    749\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pystac_client\\item_search.py:766\u001b[0m, in \u001b[0;36mItemSearch.item_collection_as_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03mGet the matching items as an item-collection-like dict.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03m    Dict : A GeoJSON FeatureCollection\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    765\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 766\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stac_io\u001b[38;5;241m.\u001b[39mget_pages(\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[0;32m    768\u001b[0m ):\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m page[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    770\u001b[0m         features\u001b[38;5;241m.\u001b[39mappend(feature)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pystac_client\\stac_api_io.py:229\u001b[0m, in \u001b[0;36mStacApiIO.get_pages\u001b[1;34m(self, url, method, parameters)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pages\u001b[39m(\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    219\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    220\u001b[0m     method: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m     parameters: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    222\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterator that yields dictionaries for each page at a STAC paging\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    endpoint, e.g., /collections, /search\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    Return:\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        Dict[str, Any] : JSON content from a single page\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollections\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pystac\\stac_io.py:198\u001b[0m, in \u001b[0;36mStacIO.read_json\u001b[1;34m(self, source, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, source: HREF, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read a dict from the given source.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    See :func:`StacIO.read_text <pystac.StacIO.read_text>` for usage of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m        given source.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_text(source, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_loads(txt)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pystac_client\\stac_api_io.py:104\u001b[0m, in \u001b[0;36mStacApiIO.read_text\u001b[1;34m(self, source, *args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m href \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(source)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_url(href):\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(href, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(href) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pystac_client\\stac_api_io.py:152\u001b[0m, in \u001b[0;36mStacApiIO.request\u001b[1;34m(self, href, method, headers, parameters)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;28mstr\u001b[39m(err))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError\u001b[38;5;241m.\u001b[39mfrom_response(resp)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAPIError\u001b[0m: <!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN' 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'><html xmlns='http://www.w3.org/1999/xhtml'><head><meta content='text/html; charset=utf-8' http-equiv='content-type'/><style type='text/css'>body {font-family:Arial; margin-left:40px; }img  { border:0 none; }#content { margin-left: auto; margin-right: auto }#message h2 { font-size: 20px; font-weight: normal; color: #000000; margin: 34px 0px 0px 0px }#message p  { font-size: 13px; color: #000000; margin: 7px 0px 0px0px}#errorref { font-size: 11px; color: #737373; margin-top: 41px }</style><title>Service unavailable</title></head><body><div id='content'><div id='message'><h2>Our services aren't available right now</h2><p>We're working to restore all services as soon as possible. Please check back soon.</p></div><div id='errorref'><span>0fUfgYwAAAADt2iLbfsajSq4Ibkcn51yaTU5aMjIxMDYwNjE0MDI5ADkyN2FiZmE2LTE5ZjYtNGFmMS1hMDlkLWM5NTlkOWExZTY0NA==</span></div></div></body></html>"
     ]
    }
   ],
   "source": [
    "# first_two_results_dict= {}\n",
    "# for key in train_dict_key_list[0:2]:\n",
    "#     first_two_results_dict[key] = functions2.try_get_sat_to_features2(train_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:20:26.491384Z",
     "start_time": "2023-02-06T00:20:26.485378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sat_train_1', 'sat_train_2'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_two_results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:20:47.028977Z",
     "start_time": "2023-02-06T00:20:47.020964Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_two_df = pd.concat(first_two_results_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:22:21.938299Z",
     "start_time": "2023-02-06T00:22:19.629477Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 51 failures out of 1706\n",
    "# first_two_df.to_pickle('../pickles/train_data_only_8_and_9/batch_1_and_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:59:42.464503Z",
     "start_time": "2023-02-06T00:32:38.283959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853 attempts. 2 failures.\n",
      "853 attempted. 7 failures.\n",
      "853 attempts. 2 failures.\n",
      "853 attempted. 10 failures.\n"
     ]
    }
   ],
   "source": [
    "three_and_four_results_dict= {}\n",
    "for key in train_dict_key_list[2:4]:\n",
    "    three_and_four_results_dict[key] = functions2.try_get_sat_to_features(train_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T01:02:46.516921Z",
     "start_time": "2023-02-06T01:02:46.505731Z"
    }
   },
   "outputs": [],
   "source": [
    "three_and_four_df = pd.concat(three_and_four_results_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T01:03:10.283465Z",
     "start_time": "2023-02-06T01:03:07.121155Z"
    }
   },
   "outputs": [],
   "source": [
    "three_and_four_df.to_pickle('../pickles/train_data_only_8_and_9/batch_3_and_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T01:03:21.891244Z",
     "start_time": "2023-02-06T01:03:21.874228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                  0\n",
       "latitude             0\n",
       "longitude            0\n",
       "date                 0\n",
       "split                0\n",
       "region               0\n",
       "severity             0\n",
       "density              0\n",
       "date_range           0\n",
       "bbox                 0\n",
       "big_crop_bbox        0\n",
       "small_crop_bbox      0\n",
       "tiny_crop_bbox       0\n",
       "item                 4\n",
       "satelite_name        4\n",
       "img_date             4\n",
       "cloud_cover(%)       4\n",
       "img_bbox             4\n",
       "has_sample_point     4\n",
       "red_mean            17\n",
       "red_median          17\n",
       "red_max             17\n",
       "red_min             17\n",
       "red_sum             17\n",
       "red_product         17\n",
       "green_mean          17\n",
       "green_median        17\n",
       "green_max           17\n",
       "green_min           17\n",
       "green_sum           17\n",
       "green_product       17\n",
       "blue_mean           17\n",
       "blue_median         17\n",
       "blue_max            17\n",
       "blue_min            17\n",
       "blue_sum            17\n",
       "blue_product        17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_and_four_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking results of first pull (a lot more missing than expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:25:33.551177Z",
     "start_time": "2023-02-06T00:25:26.661470Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle('../pickles/train_data_only_8_and_9/batch_1_and_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:28:01.330057Z",
     "start_time": "2023-02-06T00:28:01.317044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                   0\n",
       "latitude              0\n",
       "longitude             0\n",
       "date                  0\n",
       "split                 0\n",
       "region                0\n",
       "severity              0\n",
       "density               0\n",
       "date_range            0\n",
       "bbox                  0\n",
       "big_crop_bbox         0\n",
       "small_crop_bbox       0\n",
       "tiny_crop_bbox        0\n",
       "item                 35\n",
       "satelite_name        35\n",
       "img_date             35\n",
       "cloud_cover(%)       35\n",
       "img_bbox             35\n",
       "has_sample_point     35\n",
       "red_mean            211\n",
       "red_median          211\n",
       "red_max             211\n",
       "red_min             211\n",
       "red_sum             211\n",
       "red_product         455\n",
       "green_mean          211\n",
       "green_median        211\n",
       "green_max           211\n",
       "green_min           211\n",
       "green_sum           211\n",
       "green_product       454\n",
       "blue_mean           212\n",
       "blue_median         212\n",
       "blue_max            212\n",
       "blue_min            212\n",
       "blue_sum            212\n",
       "blue_product        455\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T00:28:50.409994Z",
     "start_time": "2023-02-06T00:28:50.388980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>date_range</th>\n",
       "      <th>bbox</th>\n",
       "      <th>...</th>\n",
       "      <th>green_max</th>\n",
       "      <th>green_min</th>\n",
       "      <th>green_sum</th>\n",
       "      <th>green_product</th>\n",
       "      <th>blue_mean</th>\n",
       "      <th>blue_median</th>\n",
       "      <th>blue_max</th>\n",
       "      <th>blue_min</th>\n",
       "      <th>blue_sum</th>\n",
       "      <th>blue_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aaia</td>\n",
       "      <td>35.98</td>\n",
       "      <td>-78.791686</td>\n",
       "      <td>2018-06-27</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>16943.0</td>\n",
       "      <td>2018-06-12/2018-06-27</td>\n",
       "      <td>[-79.3448638567784, 35.530359136482794, -78.23...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aaie</td>\n",
       "      <td>35.861325</td>\n",
       "      <td>-78.768321</td>\n",
       "      <td>2013-11-06</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>2</td>\n",
       "      <td>22822.0</td>\n",
       "      <td>2013-10-22/2013-11-06</td>\n",
       "      <td>[-79.32067332694183, 35.41167539133539, -78.21...</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6391.194824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.007996</td>\n",
       "      <td>72.576904</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4144.447754</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aaig</td>\n",
       "      <td>35.896233</td>\n",
       "      <td>-79.047705</td>\n",
       "      <td>2015-08-24</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>3</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>2015-08-09/2015-08-24</td>\n",
       "      <td>[-79.6002992361465, 35.4465856928807, -78.4951...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aamg</td>\n",
       "      <td>37.9718</td>\n",
       "      <td>-121.374</td>\n",
       "      <td>2015-08-18</td>\n",
       "      <td>train</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "      <td>1720200.0</td>\n",
       "      <td>2015-08-03/2015-08-18</td>\n",
       "      <td>[-121.94178501262583, 37.52230994521869, -120....</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3514.954102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.909157</td>\n",
       "      <td>53.213501</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4474.186035</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>aaoj</td>\n",
       "      <td>38.0201</td>\n",
       "      <td>-122.259</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>train</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "      <td>4080000.0</td>\n",
       "      <td>2015-06-22/2015-07-07</td>\n",
       "      <td>[-122.82715745147912, 37.57061363901757, -121....</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6965.844238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.210312</td>\n",
       "      <td>128.327942</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8013.459961</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>cpnv</td>\n",
       "      <td>35.725857</td>\n",
       "      <td>-79.126694</td>\n",
       "      <td>2014-07-23</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>4841.0</td>\n",
       "      <td>2014-07-08/2014-07-23</td>\n",
       "      <td>[-79.67810891201393, 35.27619670005493, -78.57...</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9211.830078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.191681</td>\n",
       "      <td>168.936340</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9740.267578</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>cpqg</td>\n",
       "      <td>37.3978</td>\n",
       "      <td>-120.96</td>\n",
       "      <td>2014-05-13</td>\n",
       "      <td>train</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "      <td>5760513.5</td>\n",
       "      <td>2014-04-28/2014-05-13</td>\n",
       "      <td>[-121.52342660723262, 36.94826617337809, -120....</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>cpto</td>\n",
       "      <td>37.3133</td>\n",
       "      <td>-120.892</td>\n",
       "      <td>2013-09-10</td>\n",
       "      <td>train</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "      <td>2523552.5</td>\n",
       "      <td>2013-08-26/2013-09-10</td>\n",
       "      <td>[-121.45479540786056, 36.86375974977068, -120....</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>cpwi</td>\n",
       "      <td>35.880425</td>\n",
       "      <td>-78.921187</td>\n",
       "      <td>2015-09-24</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>11618.0</td>\n",
       "      <td>2015-09-09/2015-09-24</td>\n",
       "      <td>[-79.47367143719137, 35.43077666829374, -78.36...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>cpxk</td>\n",
       "      <td>37.9792</td>\n",
       "      <td>-121.57</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>train</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "      <td>1735479.5</td>\n",
       "      <td>2016-02-21/2016-03-07</td>\n",
       "      <td>[-122.13784201571461, 37.52971051103873, -121....</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      uid   latitude  longitude       date  split region severity    density  \\\n",
       "8    aaia      35.98 -78.791686 2018-06-27  train  south        1    16943.0   \n",
       "9    aaie  35.861325 -78.768321 2013-11-06  train  south        2    22822.0   \n",
       "10   aaig  35.896233 -79.047705 2015-08-24  train  south        3   199930.0   \n",
       "12   aamg    37.9718   -121.374 2015-08-18  train   west        4  1720200.0   \n",
       "13   aaoj    38.0201   -122.259 2015-07-07  train   west        4  4080000.0   \n",
       "..    ...        ...        ...        ...    ...    ...      ...        ...   \n",
       "840  cpnv  35.725857 -79.126694 2014-07-23  train  south        1     4841.0   \n",
       "842  cpqg    37.3978    -120.96 2014-05-13  train   west        4  5760513.5   \n",
       "846  cpto    37.3133   -120.892 2013-09-10  train   west        4  2523552.5   \n",
       "849  cpwi  35.880425 -78.921187 2015-09-24  train  south        1    11618.0   \n",
       "852  cpxk    37.9792    -121.57 2016-03-07  train   west        4  1735479.5   \n",
       "\n",
       "                date_range                                               bbox  \\\n",
       "8    2018-06-12/2018-06-27  [-79.3448638567784, 35.530359136482794, -78.23...   \n",
       "9    2013-10-22/2013-11-06  [-79.32067332694183, 35.41167539133539, -78.21...   \n",
       "10   2015-08-09/2015-08-24  [-79.6002992361465, 35.4465856928807, -78.4951...   \n",
       "12   2015-08-03/2015-08-18  [-121.94178501262583, 37.52230994521869, -120....   \n",
       "13   2015-06-22/2015-07-07  [-122.82715745147912, 37.57061363901757, -121....   \n",
       "..                     ...                                                ...   \n",
       "840  2014-07-08/2014-07-23  [-79.67810891201393, 35.27619670005493, -78.57...   \n",
       "842  2014-04-28/2014-05-13  [-121.52342660723262, 36.94826617337809, -120....   \n",
       "846  2013-08-26/2013-09-10  [-121.45479540786056, 36.86375974977068, -120....   \n",
       "849  2015-09-09/2015-09-24  [-79.47367143719137, 35.43077666829374, -78.36...   \n",
       "852  2016-02-21/2016-03-07  [-122.13784201571461, 37.52971051103873, -121....   \n",
       "\n",
       "     ... green_max green_min    green_sum green_product   blue_mean  \\\n",
       "8    ...       NaN       NaN          NaN           NaN         NaN   \n",
       "9    ...     255.0       0.0  6391.194824           NaN   74.007996   \n",
       "10   ...       NaN       NaN          NaN           NaN         NaN   \n",
       "12   ...     255.0       0.0  3514.954102           NaN   69.909157   \n",
       "13   ...     255.0       0.0  6965.844238           NaN  125.210312   \n",
       "..   ...       ...       ...          ...           ...         ...   \n",
       "840  ...     255.0       0.0  9211.830078           NaN  152.191681   \n",
       "842  ...       NaN       NaN          NaN           NaN         NaN   \n",
       "846  ...       NaN       NaN          NaN           NaN         NaN   \n",
       "849  ...       NaN       NaN          NaN           NaN         NaN   \n",
       "852  ...       NaN       NaN          NaN           NaN         NaN   \n",
       "\n",
       "    blue_median blue_max blue_min     blue_sum  blue_product  \n",
       "8           NaN      NaN      NaN          NaN           NaN  \n",
       "9     72.576904    255.0      0.0  4144.447754           NaN  \n",
       "10          NaN      NaN      NaN          NaN           NaN  \n",
       "12    53.213501    255.0      0.0  4474.186035           NaN  \n",
       "13   128.327942    255.0      0.0  8013.459961           NaN  \n",
       "..          ...      ...      ...          ...           ...  \n",
       "840  168.936340    255.0      0.0  9740.267578           NaN  \n",
       "842         NaN      NaN      NaN          NaN           NaN  \n",
       "846         NaN      NaN      NaN          NaN           NaN  \n",
       "849         NaN      NaN      NaN          NaN           NaN  \n",
       "852         NaN      NaN      NaN          NaN           NaN  \n",
       "\n",
       "[503 rows x 37 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first attempt at pulling in data (could also include landsat 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.489930Z",
     "start_time": "2023-02-04T23:48:29.474983Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "# first_half_key_list = list(first_half_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.505938Z",
     "start_time": "2023-02-04T23:48:29.490933Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "\n",
    "\n",
    "# first_half_results_dict = {}\n",
    "# for n, key in enumerate(first_half_key_list):\n",
    "#     first_half_results_dict[key] = get_sat_to_features(first_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the second half of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.521447Z",
     "start_time": "2023-02-04T23:48:29.506939Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_key_list = list(second_half_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.536447Z",
     "start_time": "2023-02-04T23:48:29.522448Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_results_dict = {}\n",
    "# for key in second_half_key_list:\n",
    "#     second_half_results_dict[key] = get_sat_to_features(second_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.552447Z",
     "start_time": "2023-02-04T23:48:29.537448Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_results_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the third set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T21:23:42.776569Z",
     "start_time": "2023-02-03T21:23:42.761561Z"
    }
   },
   "source": [
    "The API is incredibly finicky. Only sat_train_10-sat_train_13 was successful. I am running another pull request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.568447Z",
     "start_time": "2023-02-04T23:48:29.553448Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_key_list = second_half_key_list[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.584447Z",
     "start_time": "2023-02-04T23:48:29.569447Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_results_dict = {}\n",
    "# for key in third_pull_key_list:\n",
    "#     third_pull_results_dict[key] = get_sat_to_features(second_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Full DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the data in a dictionary, I will concat it all together into a complete dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.600451Z",
     "start_time": "2023-02-04T23:48:29.585448Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "# First pull to DataFrame\n",
    "# first_pull_df = pd.concat(first_half_results_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.615956Z",
     "start_time": "2023-02-04T23:48:29.603451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Second pull to DataFrame\n",
    "# second_pull_df = pd.concat(second_half_results_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.631956Z",
     "start_time": "2023-02-04T23:48:29.616957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Third pull to DataFrame\n",
    "# third_pull_df = pd.concat(third_pull_results_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also need to store the pulled data as a .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.647955Z",
     "start_time": "2023-02-04T23:48:29.632956Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_pull_df.to_pickle('./first_7677_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.663222Z",
     "start_time": "2023-02-04T23:48:29.648955Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_pull_df = pd.read_pickle('./first_7677_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.678234Z",
     "start_time": "2023-02-04T23:48:29.664225Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_pull_df.to_pickle('./second_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.694258Z",
     "start_time": "2023-02-04T23:48:29.679232Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_pull_df = pd.read_pickle('./second_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.710138Z",
     "start_time": "2023-02-04T23:48:29.695262Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_df.to_pickle('./third_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.725645Z",
     "start_time": "2023-02-04T23:48:29.710642Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_df = pd.read_pickle('./third_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.740927Z",
     "start_time": "2023-02-04T23:48:29.726648Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_df = pd.concat([first_pull_df, second_pull_df, third_pull_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.756932Z",
     "start_time": "2023-02-04T23:48:29.741930Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_df.to_pickle('./full_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.772941Z",
     "start_time": "2023-02-04T23:48:29.757933Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_full = pd.read_pickle('../full_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:37.916764Z",
     "start_time": "2023-02-04T23:48:29.773949Z"
    }
   },
   "outputs": [],
   "source": [
    "# sat_test = functions.get_important_info(sat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling the the Data in Batches and Saving to .pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:37.932278Z",
     "start_time": "2023-02-04T23:48:37.917774Z"
    }
   },
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_499 = sat_test[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/0_to_500.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_999 = sat_test[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/500_to_999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000-1499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_1499 = sat_test[1000:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_1499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/1000_to_1499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1500-1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_1999 = sat_test[1500:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/1500_to_1999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000-2499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_2499 = sat_test[2000:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_2499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/2000_to_2499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2500-2999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_2999 = sat_test[2500:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_2999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/2500_to_2999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3000-3499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_3499 = sat_test[3000:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_3499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/3000_to_3499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3500-3999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_3999 = sat_test[3500:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_3999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/3500_to_3999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4000-4499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_4499 = sat_test[4000:4500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_4499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/4000_to_4499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4500-4999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_4999 = sat_test[4500:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_4999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/4500_to_4999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5000-5499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_5499 = sat_test[5000:5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_5499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/5000_to_5499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5500-5999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_5999 = sat_test[5500:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_5999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/5500_to_5999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6000-6510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_6510 = sat_test[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_5999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/6000_to_6510.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Saved Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:16.449249Z",
     "start_time": "2023-02-05T17:32:16.000024Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_499 = pd.read_pickle('../pickles/test_data/0_to_500.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:25.334954Z",
     "start_time": "2023-02-05T17:32:24.976440Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_999 = pd.read_pickle('../pickles/test_data/500_to_999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:28.932216Z",
     "start_time": "2023-02-05T17:32:28.480967Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_1499 = pd.read_pickle('../pickles/test_data/1000_to_1499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:31.050968Z",
     "start_time": "2023-02-05T17:32:30.576597Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_1999 = pd.read_pickle('../pickles/test_data/1500_to_1999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:33.917992Z",
     "start_time": "2023-02-05T17:32:32.896246Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_2499 = pd.read_pickle('../pickles/test_data/2000_to_2499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:36.921193Z",
     "start_time": "2023-02-05T17:32:36.095912Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_2999 = pd.read_pickle('../pickles/test_data/2500_to_2999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:38.720176Z",
     "start_time": "2023-02-05T17:32:37.783808Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_3499 = pd.read_pickle('../pickles/test_data/3000_to_3499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:40.885113Z",
     "start_time": "2023-02-05T17:32:39.752074Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_3999 = pd.read_pickle('../pickles/test_data/3500_to_3999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:42.081262Z",
     "start_time": "2023-02-05T17:32:41.840988Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_4499 = pd.read_pickle('../pickles/test_data/4000_to_4499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:44.478106Z",
     "start_time": "2023-02-05T17:32:43.119824Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_4999 = pd.read_pickle('../pickles/test_data/4500_to_4999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:46.892891Z",
     "start_time": "2023-02-05T17:32:45.344360Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_5499 = pd.read_pickle('../pickles/test_data/5000_to_5499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:49.692868Z",
     "start_time": "2023-02-05T17:32:49.479665Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_5999 = pd.read_pickle('../pickles/test_data/5500_to_5999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:54.148151Z",
     "start_time": "2023-02-05T17:32:52.279657Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_6510 = pd.read_pickle('../pickles/test_data/6000_to_6510.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a list of all batched DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:55.517482Z",
     "start_time": "2023-02-05T17:32:55.512484Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_pickle_list = [small_sat_test_499, small_sat_test_999, small_sat_test_1499,\n",
    "#                    small_sat_test_1999, small_sat_test_2499, small_sat_test_2999,\n",
    "#                    small_sat_test_3499, small_sat_test_3999, small_sat_test_4499,\n",
    "#                    small_sat_test_4999, small_sat_test_5499, small_sat_test_5999,\n",
    "#                    small_sat_test_6510]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing the List of DataFrames together so that I can concat to one large DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:59.478234Z",
     "start_time": "2023-02-05T17:32:59.464841Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_test_df = pd.concat(test_pickle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:33:56.528364Z",
     "start_time": "2023-02-05T17:33:56.520857Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_test_df = full_test_df.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:34:31.271807Z",
     "start_time": "2023-02-05T17:34:27.528467Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_test_df.to_pickle('../pickles/test_data/full_test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.189139Z",
     "start_time": "2023-02-04T23:48:29.174292Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Can use this if I decide to use multiple satelitte images\n",
    "# def get_sat_info(df):\n",
    "\n",
    "#     '''\n",
    "#     input a dataframe and get a dictionary with satellite information for each row in the dataframe\n",
    "#     '''\n",
    "    \n",
    "#     sat_dict = {}\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "#         # Get all satellite images\n",
    "#         search = catalog.search(collections=[\"sentinel-2-l2a\", \"landsat-c2-l2\"],\n",
    "#                                 bbox=row['bbox'],\n",
    "#                                 datetime=row['date_range'],\n",
    "#                                 query={'eo:cloud_cover': {'lt':100}}\n",
    "#     )\n",
    "\n",
    "\n",
    "#         # Going through Satellite info\n",
    "\n",
    "#         # search for sat images and create a dataframe with results for one sample\n",
    "# #         search_items = [item for item in search.get_all_items()]\n",
    "#         search_items = [item for item in search.item_collection()]\n",
    "\n",
    "\n",
    "#         pic_details = []\n",
    "#         for pic in search_items:\n",
    "#             pic_details.append(\n",
    "#             {\n",
    "#             'item': pic,\n",
    "#             'satelite_name':pic.collection_id,\n",
    "#             'img_date':pic.datetime.date(),\n",
    "#             'cloud_cover(%)': pic.properties['eo:cloud_cover'],\n",
    "#             'img_bbox': pic.bbox,\n",
    "#             'min_long': pic.bbox[0],\n",
    "#             \"max_long\": pic.bbox[2],\n",
    "#             \"min_lat\": pic.bbox[1],\n",
    "#             \"max_lat\": pic.bbox[3]\n",
    "#             }\n",
    "#             )\n",
    "\n",
    "#         temp_df = pd.DataFrame(pic_details)\n",
    "\n",
    "#         # Check to make sure sample location is actually within sat image\n",
    "#         temp_df['has_sample_point'] = (\n",
    "#             (temp_df.min_lat < row.latitude)\n",
    "#             & (temp_df.max_lat > row.latitude)\n",
    "#             & (temp_df.min_long < row.longitude)\n",
    "#             & (temp_df.max_long > row.longitude)\n",
    "#         )\n",
    "\n",
    "#         temp_df = temp_df[temp_df['has_sample_point'] == True]\n",
    "#         sat_dict[row['uid']] = temp_df\n",
    "        \n",
    "#     return sat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.205143Z",
     "start_time": "2023-02-04T23:48:29.190140Z"
    }
   },
   "outputs": [],
   "source": [
    "# # delete comments for prints (# is all on left edge)\n",
    "# def pick_best_sat(df, sat_dict):\n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe and dictionary of satellite images and returns a dataframe with the best satellite image\n",
    "#     '''\n",
    "    \n",
    "#     # picking the best\n",
    "#     # inputs would need to be df and dictionary\n",
    "#     best_sat_df = pd.DataFrame()\n",
    "#     row_count=0\n",
    "#     invalid_sats = 0\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "#         name = row['uid']\n",
    "#         temp_df = sat_dict[name]\n",
    "#         temp_df = temp_df.reset_index()\n",
    "#         # checking to see if there's only one image and adding it to df if so\n",
    "#         if len(temp_df) == 1:\n",
    "# #             print('only one satellite')\n",
    "#             temp_df = temp_df.reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#             row_count+=1\n",
    "\n",
    "#         # checking if no images\n",
    "#         elif len(temp_df) == 0:\n",
    "#             invalid_sats +=1\n",
    "#             row = pd.DataFrame(row).T.reset_index()\n",
    "#             row = row.set_index(pd.Series(row_count)).drop('index', axis=1)\n",
    "#             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#             row_count+=1\n",
    "# #             print('no satellite images')\n",
    "#             continue\n",
    "\n",
    "#         # There are many satellite images, need to narrow it down\n",
    "#         else:\n",
    "# #             print('many sats')\n",
    "#             # first checking for any sentinel satelites\n",
    "#             if len(temp_df[temp_df['satelite_name'].str.contains('entinel')]) >0:\n",
    "#                     temp_df = temp_df[temp_df['satelite_name'].str.contains('entinel')]\n",
    "\n",
    "#                     # if only one sentinel, add to df and move on\n",
    "#                     if len(temp_df) == 1:\n",
    "# #                         print('\\tonly one sentinal')\n",
    "#                         temp_df = temp_df.reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                         row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                         row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                         best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                         row_count+=1\n",
    "#                     # if many sentinel, check for images with low cloud cover\n",
    "#                     else:\n",
    "# #                         print('\\tmany sentinel')\n",
    "#                         # checking for clouds less than 30%\n",
    "#                         if len(temp_df[temp_df['cloud_cover(%)'] <= 30]) >0:\n",
    "# #                             print('\\t\\tsentinal cloud cover lower than 30%')\n",
    "#                             temp_df = temp_df[temp_df['cloud_cover(%)'] <= 30]\n",
    "\n",
    "#                             # add the row with the closest date\n",
    "#                             temp_df = temp_df.sort_values('img_date', ascending=False).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                             temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                             row_count+=1\n",
    "#                         else:\n",
    "#                             # If there's only images with a clouds over 30%, \n",
    "#                             # pick the one with the least clouds\n",
    "# #                             print('\\t\\tvery cloudy sentinel')\n",
    "#                             temp_df = temp_df.sort_values('cloud_cover(%)', ascending=True).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                             temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                             row_count+=1\n",
    "\n",
    "#             else:\n",
    "# #                 print('\\tno sentinal')\n",
    "#                 if len(temp_df[temp_df['cloud_cover(%)'] <= 30]) >0:\n",
    "# #                     print('\\t\\tlandsat cloud cover lower than 30%')\n",
    "#                     temp_df = temp_df[temp_df['cloud_cover(%)'] <= 30]\n",
    "\n",
    "#                     # add the row with the closest date\n",
    "#                     temp_df = temp_df.sort_values('img_date', ascending=False).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                     temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                     row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                     row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                     best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                     row_count+=1\n",
    "#                 else:\n",
    "#                     # If there's only images with a clouds over 30%, \n",
    "#                     # pick the one with the least clouds\n",
    "# #                     print('\\t\\tvery cloudy landsat')\n",
    "#                     temp_df = temp_df.sort_values('cloud_cover(%)', ascending=True).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                     temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                     row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                     row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                     best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                     row_count+=1\n",
    "\n",
    "\n",
    "\n",
    "#     print(f'{len(df)} attempts. {invalid_sats} failures.')\n",
    "#     return best_sat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.220931Z",
     "start_time": "2023-02-04T23:48:29.206143Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_arrays_from_sats(df):\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe with satellites in it and get a dictionary with arrays \n",
    "#     that came from cropped images around the sample area\n",
    "#     '''\n",
    "\n",
    "# # Now to get images from the satellites\n",
    "#     array_dict = {}\n",
    "#     scaler = functions.MinMaxScaler3D(feature_range=(0,255))\n",
    "#     error_count = 0\n",
    "#     attempt_count = 0\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             attempt_count +=1\n",
    "#         # checking to see which satellite it came from\n",
    "#             if 'sentinel' in row['satelite_name']:\n",
    "#                 # Setting tiny crop box for image\n",
    "#                 minx, miny, maxx, maxy = row['tiny_crop_bbox']\n",
    "#                 # getting the image\n",
    "#                 image = rioxarray.open_rasterio(pc.sign(row['item'].assets[\"visual\"].href)).rio.clip_box(\n",
    "#                         minx=minx,\n",
    "#                         miny=miny,\n",
    "#                         maxx=maxx,\n",
    "#                         maxy=maxy,\n",
    "#                         crs=\"EPSG:4326\",\n",
    "#                     )\n",
    "\n",
    "#                 image_array = image.to_numpy()\n",
    "#                 img_array_trans = np.transpose(image_array, axes=[1, 2, 0])\n",
    "#                 # storing array of image in dictionary\n",
    "#                 array_dict[row['uid']] = img_array_trans\n",
    "\n",
    "#             else:\n",
    "#                 # getting the image from the LandSat satellite\n",
    "#                 minx, miny, maxx, maxy = row['tiny_crop_bbox']\n",
    "#                 image = odc.stac.stac_load(\n",
    "#                         [pc.sign(row['item'])], bands=[\"red\", \"green\", \"blue\"], bbox=[minx, miny, maxx, maxy]\n",
    "#                     ).isel(time=0)\n",
    "\n",
    "#                 image_array = image[[\"red\", \"green\", \"blue\"]].to_array()\n",
    "#                 img_array_trans = np.transpose(image_array.to_numpy(), axes=[1, 2, 0])\n",
    "#                 # scaling the image so its the same scale as the sentinel ones\n",
    "#                 scaled_img = scaler.fit_transform(img_array_trans)\n",
    "#         #         int_scaled_img = scaled_img.astype(int)\n",
    "#                 # storing array of image in dictionary\n",
    "#                 array_dict[row['uid']] = scaled_img\n",
    "                \n",
    "\n",
    "#         except:\n",
    "#             error_count +=1\n",
    "            \n",
    "            \n",
    "#     print(f'{attempt_count} attempted. {error_count} failures.')\n",
    "#     return array_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.236936Z",
     "start_time": "2023-02-04T23:48:29.221932Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_features(df, img_arrays):\n",
    "#     '''\n",
    "#     input a dataframe and a list of integers and create features from arrays\n",
    "#     '''\n",
    "#     feature_df = pd.DataFrame()\n",
    "#     for index in range(len(img_arrays.keys())):\n",
    "#         feature_dict = {}\n",
    "#         key =list(img_arrays.keys())[index]\n",
    "# #         row = df.iloc[index]\n",
    "#         temp_array = img_arrays[key]\n",
    "#         for n, color in enumerate(['red', 'green', 'blue']):\n",
    "#             feature_dict['uid'] = key\n",
    "#             feature_dict[f'{color}_mean'] = np.mean(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_median'] = np.median(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_max'] = np.max(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_min'] = np.min(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_sum'] = np.sum(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_product'] = np.prod(temp_array[:,:,n])\n",
    "#         feature_df = pd.concat([feature_df, pd.DataFrame(feature_dict, index=[index])], )\n",
    "\n",
    "#     feature_df = df.merge(feature_df, how='outer', on='uid')\n",
    "#     return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.252937Z",
     "start_time": "2023-02-04T23:48:29.237937Z"
    }
   },
   "outputs": [],
   "source": [
    "# # A function to get it all in one\n",
    "# def get_sat_to_features(df):\n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe of raw data and get sat images, convert to arrays, and turn into features.\n",
    "#     '''\n",
    "#     catalog = Client.open(\n",
    "#     \"https://planetarycomputer.microsoft.com/api/stac/v1\", modifier=pc.sign_inplace\n",
    "#     )\n",
    "    \n",
    "#     # get sat info\n",
    "#     satelite_dict = get_sat_info(df)\n",
    "    \n",
    "#     # pick best sat\n",
    "#     single_df = pick_best_sat(df, satelite_dict)\n",
    "    \n",
    "#     # get image arrays from best sats\n",
    "#     img_arrays = get_arrays_from_sats(single_df)\n",
    "    \n",
    "#     # get a dataframe with relevant features\n",
    "#     feature_df = get_features(single_df, img_arrays)\n",
    "    \n",
    "#     return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.268948Z",
     "start_time": "2023-02-04T23:48:29.253937Z"
    }
   },
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     '''\n",
    "#     input dataframe with all data and clean it.\n",
    "#     '''\n",
    "#     # only keeping cols that I need\n",
    "#     model_df = df[['date', 'latitude', 'longitude', 'season', 'img_date',\n",
    "#             'red_mean', 'red_median', 'red_max', 'red_min','red_sum',\n",
    "#             'red_product', 'green_mean', 'green_median', 'green_max',\n",
    "#             'green_min', 'green_sum', 'green_product', 'blue_mean',\n",
    "#             'blue_median','blue_max', 'blue_min', 'blue_sum', 'blue_product', 'severity']]\n",
    "#     # dropping nulls\n",
    "#     model_df = model_df.dropna()\n",
    "#     # converting to correct type\n",
    "#     model_df['date'] = model_df['date'].apply(lambda x: datetime.date(x))\n",
    "#     # getting difference from image date to sample date and creating feature\n",
    "#     model_df['days_from_sat_to_sample'] = model_df['date'] - model_df['img_date']\n",
    "#     # converting to int\n",
    "#     model_df['days_from_sat_to_sample'] = model_df['days_from_sat_to_sample'].dt.days\n",
    "#     # converting from datetime to an int\n",
    "#     model_df['date'] = model_df['date'].apply(lambda x: x.toordinal())\n",
    "#     model_df['img_date'] = model_df['img_date'].apply(lambda x: x.toordinal())\n",
    "#     # converting from string to float\n",
    "#     model_df['latitude'] = model_df['latitude'].apply(lambda x: x.astype(float))\n",
    "#     model_df['longitude'] = model_df['longitude'].apply(lambda x: x.astype(float))\n",
    "    \n",
    "#     # One hot encoding seasons\n",
    "#     ohe = OneHotEncoder(sparse=False)\n",
    "#     seasons = ohe.fit_transform(model_df[['season']])\n",
    "#     cols = ohe.get_feature_names_out()\n",
    "#     # converting new ohe to dataframe\n",
    "#     seasons = pd.DataFrame(seasons, columns=cols, index=model_df.index)\n",
    "#     model_ohe = pd.concat([model_df.drop('season', axis=1), seasons], axis=1)\n",
    "#     return model_ohe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
